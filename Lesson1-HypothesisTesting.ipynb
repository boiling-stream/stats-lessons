{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "### Overview\n",
    "\n",
    "Say you have a novel theory about the way the world works.  You decide to test your idea by collecting evidence.  As you go about gathering data you find some pieces of evidence that support your hypothesis but also you turn up many that do not.  When all is said and done, however, the sum of the data appears to suggest that your idea was correct!  Given the scatter in your measurements, however, how confident can you be that your positive result was not a fluke?\n",
    "\n",
    "_Hypothesis testing is used to evaluate the statistical significance of a positive result._\n",
    "\n",
    "It works by defining a null hypothesis that is mutually exclusive with your idea about the world.  Given the evidence, a statistical test is performed to calculate how likely it is for the null hypothesis to be true.  If it is unlikely, your idea is supported.  If it is likely, your positive result isn't statistically significant.\n",
    "\n",
    "The words \"likely\" and \"unlikely\" here are vague.  In practice, we define a threshold value that discriminates between the two.  A commonly used, but by no means universal, threshold is 95%.  That is, it may be fair to deem the null hypothesis to be \"unlikely\" if we are at least 95% certain that the evidence does not support it.  It is important to note, however, that in this case 1 out of 20 trials ought to result in a rejection (falsely) of the null hypothesis just by random chance.  This relates to the concepts of precision and recall that are discussed at the end of this lesson.\n",
    "\n",
    "### Example\n",
    "\n",
    "To make this discussion more concrete, let's formulate null and alternative hypotheses for a real-life example.  Art and Jesse are business partners who run a small cash-only donut shop.  Until recently, their best-selling item was Art's Old Fashioned Donut, but Jesse recently added a new option to the menu---the Boston Creme Eclair, which has become quite popular.  Art, feeling somewhat snubbed, wishes to show that the increased popularity the the new donut is actually costing the shop money, because the profit on each sold Boston Creme is smaller than on each Old Fashioned.\n",
    "\n",
    "Art knows with a great (in fact, perfect) degree of certainty that the profit on one Old Fashioned donut is $2.93, but he does not have the same information about Jesse's eclair.  Therefore, he painstakingly tracks the cost of making 100 individual eclairs at random.  He obtains the following distribution:\n",
    "\n",
    "![Distribution](distribution.png)\n",
    "\n",
    "Art is pleased to see that the average profit is smaller than $2.93 for the sample of 100 eclairs, but before he brings this finding to Jesse, he wants to be quite certain (95%) that his result is not a fluke.  He needs to do a hypothesis test where:\n",
    "\n",
    "\\begin{align}\n",
    "H_0: \\mu = \\$2.93 \\\\\n",
    "H_A: \\mu < \\$2.93 \n",
    "\\end{align}\n",
    "\n",
    "#### The $t$-test\n",
    "\n",
    "To reject the null hypothesis, Art must calculate how likely it is that his result occurred by random chance.  Assuming that his data are independent and identically normally distributed (IID)---statistical jargon meaning that none of the measurements are correlated and that they all derive from the same distribution---, Art can do this by conducting a $t$-test.\n",
    "\n",
    "This is done by computing the $t$-statistic of the dataset and comparing it with the value on the cumulative distribution function ([CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function)) of the $t$-distribution for $n$ - 1 degrees of freedom. The t-statistic for parameter $\\beta$ is given by \n",
    "\n",
    "\\begin{equation}\n",
    "t_\\hat{\\beta} = \\frac{\\hat \\beta - \\beta_0}{SE(\\hat B)}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat \\beta$ is the estimator of $\\beta$, $\\beta_0$ is the reference value of $\\beta$, and $SE$ is the standard error of the measurement.  In Art's case, he should be looking at the *left tail* of the t-distribution because he is seeking to show that the mean margin is *less than* \\$2.93. Therefore, if the CDF is $<0.05$ for the $t$-statistic, Art can reject the null hypothesis with 95% confidence that the result didn't occur by random chance.  The value 0.05 is called the *significance level* of the test.\n",
    "\n",
    "Let's look at the code that generated Art's distribution of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy import mean, std\n",
    "from scipy.stats import norm\n",
    "\n",
    "#Model Art's measurements as a normally distributed random variate with prescribed mean and variance\n",
    "true_mean = 2.82\n",
    "true_variance = 0.56\n",
    "\n",
    "seed(313)\n",
    "x = norm.rvs(loc=true_mean, scale=true_variance**0.5, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical programming languages have packages that can conduct the t-test under the hood, but let's go through the calculation ourselves.  First, compute the $t$-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The t-statistic is    -1.258419\n"
     ]
    }
   ],
   "source": [
    "#The mean margin on Old Fashioned\n",
    "reference_mean = 2.93\n",
    "\n",
    "se     = std(x)/sqrt(len(x))\n",
    "tstat  = (mean(x)-reference_mean)/se\n",
    "\n",
    "print(\"The t-statistic is %12f\" % tstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the CDF for the t-distribution for the value of the $t$-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value is     0.105600\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "pvalue = t.cdf(tstat,df=len(x)-1)\n",
    "\n",
    "print('The p-value is %12f' % pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **$p$-value** is the probability that the result occurred by random chance.  Since it is greater than 0.05 in this example, Art cannot reject the null hypothesis.\n",
    "\n",
    "This example demonstrates a **type II error** in hypothesis testing because, unlike Art, we contrived the example and know that the null hypothesis was false.  This is in contrast to a **type I error** where a null hypothesis is incorrectly rejected.  Let us consider what could have Art done to make his hypothesis test more (or less) accurate using the concepts of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "Precision = $\\frac{\\mathrm{True\\,Positives}}{\\mathrm{True\\,Positives} + \\mathrm{False\\,Positives}}$\n",
    "\n",
    "Recall = $\\frac{\\mathrm{True\\,Positives}}{\\mathrm{True\\,Positives} + \\mathrm{False\\,negatives}}$\n",
    "\n",
    "A True Positive occurs when the null hypothesis is correctly rejected; a True Negative occurs with the null hypothesis is correctly *not* rejected.\n",
    "\n",
    "Using code, let's repeat Art's experiment 10,000 times to see how often he gets a negative result when drawing samples from the same underlying distribution.  To make things more interesting, there is a 50 percent chance that we'll draw data drawing data from the Eclair distribution (true result is positive) and Old Fashioned distribution (true result is negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signif. level 0.05, Prob +ve result 0.50:\n",
      "     True  positives =     2204\n",
      "     True  negatives =     4751\n",
      "     False positives =      250\n",
      "     False negatives =     2795\n",
      "     Precision       =       89%\n",
      "     Recall          =       44%\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "def calc_precision_recall(nsamples = 100, signif_level = 0.05, p_positive=0.5):\n",
    "    from numpy.random import rand #uniform random variate between 0 and 1\n",
    "    \n",
    "    n_iters = 10000\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        seed(313+iteration)\n",
    "        \n",
    "        if rand() < p_positive:\n",
    "            x = norm.rvs(loc=true_mean, scale=true_variance**0.5, size=nsamples)\n",
    "            truth = 'positive'\n",
    "        else:\n",
    "            x = norm.rvs(loc=reference_mean, scale=true_variance**0.5, size=nsamples)\n",
    "            truth = 'negative'\n",
    "            \n",
    "        se     = std(x)/sqrt(len(x))\n",
    "        tstat  = (mean(x)-reference_mean)/se\n",
    "        pvalue = t.cdf(tstat,df=len(x)-1)\n",
    "\n",
    "        if truth == 'positive' and pvalue < signif_level:\n",
    "            true_positives += 1\n",
    "        if truth == 'positive' and pvalue >= signif_level:\n",
    "            false_negatives += 1\n",
    "        if truth == 'negative' and pvalue < signif_level:\n",
    "            false_positives += 1\n",
    "        if truth == 'negative' and pvalue >= signif_level:\n",
    "            true_negatives += 1\n",
    "            \n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) \n",
    "    recall    = true_positives / (true_positives + false_negatives) \n",
    "\n",
    "    print('Signif. level %4.2f, Prob +ve result %4.2f:' % (signif_level, p_positive))\n",
    "    print('     True  positives = %8d' % true_positives)\n",
    "    print('     True  negatives = %8d' % true_negatives)\n",
    "    print('     False positives = %8d' % false_positives)\n",
    "    print('     False negatives = %8d' % false_negatives)\n",
    "    print('     Precision       = %8d' % (precision*100.) + '%')\n",
    "    print('     Recall          = %8d' % (recall*100.) + '%')\n",
    "    print('********************************')\n",
    "    \n",
    "calc_precision_recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Art's test was fairly precise---only about 11% of the time would he have falsely obtained a positive result.  Recall, however, was poor.  He was more likely to falsely obtain a negative result than correctly obtain a positive one.  In light of this, it is illustrative to explore how precision and recall are affected by the significance level of the $t$-test.  Let's repeat the above analysis varying $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signif. level 0.01, Prob +ve result 0.50:\n",
      "     True  positives =      965\n",
      "     True  negatives =     4940\n",
      "     False positives =       61\n",
      "     False negatives =     4034\n",
      "     Precision       =       94%\n",
      "     Recall          =       19%\n",
      "********************************\n",
      "Signif. level 0.05, Prob +ve result 0.50:\n",
      "     True  positives =     2204\n",
      "     True  negatives =     4751\n",
      "     False positives =      250\n",
      "     False negatives =     2795\n",
      "     Precision       =       89%\n",
      "     Recall          =       44%\n",
      "********************************\n",
      "Signif. level 0.10, Prob +ve result 0.50:\n",
      "     True  positives =     2947\n",
      "     True  negatives =     4522\n",
      "     False positives =      479\n",
      "     False negatives =     2052\n",
      "     Precision       =       86%\n",
      "     Recall          =       58%\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "calc_precision_recall(signif_level=0.01)\n",
    "calc_precision_recall(signif_level=0.05)\n",
    "calc_precision_recall(signif_level=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the number of false positives increases as the significance level, $\\alpha$, of the statistical test is increased. Generally speaking, an optimal test seeks to consider both precision and recall.  Tuning $\\alpha$ to a very small value would give high precision, but virtually no positive results would be returned.  Conversely, assigning $\\alpha$ a high value would improve recall, but at the expense of an increased proportion of false positives.\n",
    "\n",
    "Let's rerun the calculation above with $\\alpha=0.05$, varying the probability that the true result is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signif. level 0.05, Prob +ve result 0.50:\n",
      "     True  positives =     2204\n",
      "     True  negatives =     4751\n",
      "     False positives =      250\n",
      "     False negatives =     2795\n",
      "     Precision       =       89%\n",
      "     Recall          =       44%\n",
      "********************************\n",
      "Signif. level 0.05, Prob +ve result 0.10:\n",
      "     True  positives =      444\n",
      "     True  negatives =     8519\n",
      "     False positives =      460\n",
      "     False negatives =      577\n",
      "     Precision       =       49%\n",
      "     Recall          =       43%\n",
      "********************************\n",
      "Signif. level 0.05, Prob +ve result 0.01:\n",
      "     True  positives =       48\n",
      "     True  negatives =     9388\n",
      "     False positives =      506\n",
      "     False negatives =       58\n",
      "     Precision       =        8%\n",
      "     Recall          =       45%\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "calc_precision_recall(signif_level=0.05, p_positive=0.5)\n",
    "calc_precision_recall(signif_level=0.05, p_positive=0.1)\n",
    "calc_precision_recall(signif_level=0.05, p_positive=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major limitation of hypothesis testing is that it does not to account for prior information.  Applying it to hypotheses that are highly unlikely will result in a false positive far more often than a true one.  Bayesian inference, which will be covered in the next lesson, is an alternative approach to hypothesis testing that accounts for prior information.\n",
    "\n",
    "[Related Reading at Five Thirty Eight](https://fivethirtyeight.com/features/how-shoddy-statistics-found-a-home-in-sports-research/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
